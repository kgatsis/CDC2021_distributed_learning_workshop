
<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <!-- The above 3 meta tags *must* come first in the head; any other head content must come *after* these tags -->
    <meta name="description" content="">    
    <meta name="keywords" content="***ADD KEYWORDS HERE">
    <meta name="author" content="Konstantinos Gatsis">
    <link rel="icon" href="favicon.ico">

    <title> Workshop on Learning for Control (CDC 2018)</title>

    <!-- Bootstrap core CSS -->
    <link href="assets/css/bootstrap.min.css" rel="stylesheet">

    <!-- IE10 viewport hack for Surface/desktop Windows 8 bug -->
    <link href="assets/css/ie10-viewport-bug-workaround.css" rel="stylesheet">

    <!-- Custom styles for this template -->
    <link href="assets/css/workshop18-style.css" rel="stylesheet">

    <!-- Just for debugging purposes. Don't actually copy these 2 lines! -->
    <!--[if lt IE 9]><script src="../../assets/js/ie8-responsive-file-warning.js"></script><![endif]-->
    <script src="assets/js/ie-emulation-modes-warning.js"></script>

    <!-- <script src="assets/js/anchor-with-navbar-fix.js"></script> -->

    <!-- HTML5 shim and Respond.js for IE8 support of HTML5 elements and media queries -->
    <!--[if lt IE 9]>
      <script src="https://oss.maxcdn.com/html5shiv/3.7.2/html5shiv.min.js"></script>
      <script src="https://oss.maxcdn.com/respond/1.4.2/respond.min.js"></script>
    <![endif]-->
    
    <script type="text/javascript">
    <!--
    function toggleAbstract(divid) {
      var x = document.getElementById(divid);
      if (x.style.display === "none") {
          x.style.display = "block";
      } else {
          x.style.display = "none";
      }
    }
    -->
  </script>    
  </head>

  <body>
    <nav class="navbar navbar-inverse navbar-default navbar-fixed-top">
      <div class="container">
      <!--
        <div class="navbar-header">
          <button type="button" class="navbar-toggle" data-toggle="collapse" data-target=".navbar-collapse">
            <span class="sr-only">Toggle navigation</span>
            <span class="icon-bar"></span>
            <span class="icon-bar"></span>
            <span class="icon-bar"></span>
          </button>
          <a class="navbar-brand" href="#">n ame </br>conference name Workshop</a>
        </div>
        -->
        <div class="collapse navbar-collapse">
          <ul class="nav navbar-nav">
            <li><a href="#overview">Overview</a></li>
            <li><a href="#dates">Location</a></li>
            <li><a href="#registration">Registration</a></li>
            <li><a href="#speakers">Invited Speakers</a></li>
            <li><a href="#schedule">Schedule</a></li>
            <li><a href="#contact">Organizers</a></li>
            <!--<li><a href="#support">Travel Support</a></li>-->
          </ul>
        </div><!--/.nav-collapse -->
      </div>
    </nav>

    <div class="container body">
      
      <div style="height:20px;"></div>

      
      <div class="starter-template">
        <img id="topimage" src="assets/fig/banner_cdc_2018_new.jpg">
      </div>
	  
      <!------------------ Overview ---------------------------->
      <div class="page-header" id="overview">
      <h1>Workshop on Learning for Control</h1>
	  <h3><a href="https://cdc2018.ieeecss.org/index.php">57th IEEE Conference on Decision and Control</a><br>
		  Miami Beach, Florida, December 16, 2018
		  <br>Room: Splash 11-12
      </h3>
	  </div>
        <p>  Over the past two decades, advances in computing and communications have resulted in the creation, transmission and storage of data from all sectors of society. Over the next decade, the biggest generator of data is expected to be Internet-of-Things devices which sense and control the physical world. This explosion of data that is emerging from the physical world requires a rapprochement of areas such as machine learning, control theory, and optimization. The availability and scale of data, both temporal and spatial, brings a wonderful opportunity for our community to both advance the theory of control systems in a more data-driven fashion, as well as have a broader industrial and societal impact.</p>

		<p>There are various challenges on the interface between the control community and the machine learning community.  The goal of our workshop is to focus on what new ideas, approaches or questions can arise when learning theory is applied to control problems.In particular, our workshop goals are:</p>
		<ul>
		<li> Present state-of-the-art results in the theory and application of Learning for Control, including topics such as statistical learning for control, reinforcement learning for control, online and safe learning for control </li>
		<li> Bring together some of the leading researchers across the fields in order to promote cross-fertilization of results, tools, and ideas, and stimulate further progress in the area </li>
		<li>Attract new researchers in these exciting problems, creating a larger yet focused community that thinks rigorously across the disciplines and ask new questions</li>
		</ul>
		
		<p>We are delighted to have assembled a world-class team of leading researchers working on the interface between machine learning and control.
		</p>
    
    
      <!------------------ Date ---------------------------->
      <div class="page-header" id="dates">
        <h2>Date and Location</h2>
      </div>
    <p>The workshop will take place on <b>Sunday December 16, 2018</b> during the <a href="https://cdc2018.ieeecss.org/index.php"> 
      57th IEEE Conference on Decision and Control </a>
      at the <a href="https://fontainebleau.com/">Fontainebleau</a> in Miami Beach, FL, USA.</p>
      
	<p> For accomodation information please visit the <a href="https://cdc2018.ieeecss.org/logistics.php">conference page</a>.</p>

					
            <!------------------ Registration ---------------------------->

      <div class="page-header" id="registration">
        <h2>Registration</h2>
      </div>
      <p> Registration for the workshop can be made through <a href="https://cdc2018.ieeecss.org/registration.php"> this link</a> 
        at the 57th IEEE Conference on Decision and Control website. Please note that <b>only people who have registered for the 
        conference can register for the workshop</b>. The conference early registration rate is till October 1st. The workshop fees are as follows:</p>
		


        
      <table class="other", width="20%">
        <tbody>  
          <tr>
            <th> Category </th>
            <th> Fee </th>
            <tr>
			<td> Member</td><td>170 USD	 </td>
            </tr>
			<tr>
			<td> Non-Member</td><td>170 USD </td>
            </tr>
			<tr>
			<td> Life Member</td><td>85 USD </td>
            </tr>
			<tr>
			<td> Student</td><td>85 USD  </td>
            </tr>
			<tr>
			<td> Retiree</td><td>85 USD  </td>
            </tr>
          </tr>    
                       
        </tbody>
      </table>
            


      <!------------------ Speakers ---------------------------->
		<div class="page-header" id="speakers">
        <h2>Keynote Speaker</h2>
      </div>  
	  
	  <figure>
			<img src="assets/fig/Jordan.jpg" alt="Jordan">
			<figcaption><a href="https://people.eecs.berkeley.edu/~jordan/">Michael I. Jordan</a> <br>University of California, Berkeley </figcaption>
		</figure>
		
      <div class="page-header" id="speakers2">
        <h2>Invited Speakers</h2>
      </div>   
		<figure>
			<img src="assets/fig/Bertsekas.jpg" alt="Bertsekas">
			<figcaption><a href="http://www.mit.edu/~dimitrib/home.html">Dimitri P. Bertsekas</a> <br>Massachusetts Institute of Technology </figcaption>
		</figure>
		<figure>
			<img src="assets/fig/Borrelli.png" alt="Borrelli">
			<figcaption><a href="http://www.me.berkeley.edu/people/faculty/francesco-borrelli">Francesco Borrelli</a> <br>University of California, Berkeley</figcaption>
		</figure>
		<figure>
			<img src="assets/fig/Calafiore.jpg" alt="Calafiore">
			<figcaption><a href="http://staff.polito.it/giuseppe.calafiore/Home_Page/Home.html">Giuseppe Carlo Calafiore</a> <br>Politecnico di Torino </figcaption>
		</figure>
		<figure>
			<img src="assets/fig/Fazel.jpg" alt="Fazel">
			<figcaption><a href="https://faculty.washington.edu/mfazel/">Maryam Fazel</a> <br>University of Washington</figcaption>
		</figure>
		<br>
		
	
		
		<figure>
			<img src="assets/fig/Lewis.jpg" alt="Lewis">
			<figcaption><a href="https://www.uta.edu/utari/acs/">Frank L. Lewis</a> <br>University of Texas at Arlington </figcaption>
		</figure>
		<figure>
			<img src="assets/fig/Recht.jpg" alt="Recht">
			<figcaption><a href="https://people.eecs.berkeley.edu/~brecht/">Benjamin Recht</a> <br>University of California, Berkeley </figcaption>
		</figure>
		<figure>
			<img src="assets/fig/Schoellig.jpg" alt="Schoellig">
			<figcaption><a href="http://www.dynsyslab.org/prof-angela-schoellig/">Angela Schoellig</a> <br>University of Toronto  </figcaption>
		</figure>
		<figure>
			<img src="assets/fig/Tomlin.jpg" alt="Tomlin">
			<figcaption><a href="https://people.eecs.berkeley.edu/~tomlin/">Claire J. Tomlin</a> <br>University of California, Berkeley </figcaption>
		</figure>
				<br>

				

		

		<figure>
			<img src="assets/fig/Vidal.jpg" alt="Vidal">
			<figcaption><a href="http://cis.jhu.edu/~rvidal/">Rene Vidal</a> <br>Johns Hopkins University </figcaption>
		</figure>
		
<!--     <ul>
		<li> <a href="http://www.mit.edu/~dimitrib/home.html">Dimitri P. Bertsekas</a> (Massachusetts Institute of Technology) </li>
		<li> <a href="http://www.me.berkeley.edu/people/faculty/francesco-borrelli">Francesco Borrelli</a> (University of California, Berkeley) </li>
		<li> <a href="http://staff.polito.it/giuseppe.calafiore/Home_Page/Home.html">Giuseppe Carlo Calafiore</a> (Politecnico di Torino) </li>

		<li> <a href="https://faculty.washington.edu/mfazel/">Maryam Fazel</a> (University of Washington) </li>
		
        <li> <a href="https://people.eecs.berkeley.edu/~jordan/">Michael I. Jordan</a> (University of California, Berkeley) </li>
		<li> <a href="https://www.uta.edu/utari/acs/">Frank L. Lewis</a> (University of Texas at Arlington) </li>

		<li> <a href="https://people.eecs.berkeley.edu/~brecht/">Benjamin Recht</a> (University of California, Berkeley) </li>
		
		<li> <a href="http://www.dynsyslab.org/prof-angela-schoellig/">Angela Schoellig</a> (University of Toronto) </li>
		<li> <a href="https://people.eecs.berkeley.edu/~tomlin/">Claire J. Tomlin</a> (University of California, Berkeley) </li>
		
    </ul> -->
    
 
      <!------------------ Schedule ---------------------------->

      <div class="page-header" id="schedule">
        <h2>Schedule</h2>
    </div> 
      <table class="schedule" style="width:98%">
        <tbody>  
          <tr>
            <th> Time </th>
            <th> Topic </th>
          </tr>
            <tr>
                <td> 8:30 - 8:45 </td>
                <td><strong>Opening remarks by organizers</strong></td>
          </tr>
          <tr>
            <td> 8:45 - 10:30 </td>
            <td> <p><strong>Session 1: Reinforcement Learning for Control</strong> - Chair: Pramod P. Khargonekar</p>
			<!--<br>
			</td>
		</tr>
		
            <tr>
			<td>  </td>
			<td> -->
			<p> 8:45 - 9:20 <a href="javascript:toggleAbstract('Bertsekas')">Aggregation, Rollout, Model Predictive Control, and Enhanced Policy Iteration in Reinforcement Learning</a> -
			<a href="http://www.mit.edu/~dimitrib/home.html">Dimitri P. Bertsekas</a> (Massachusetts Institute of Technology) </p>
            
            <div id="Bertsekas" style="display:none">
            <p><strong>Abstract: </strong> We discuss a new aggregation framework for approximate dynamic programming, which provides a connection with rollout algorithms, model predictive control, approximate policy iteration, and other single and multistep lookahead methods. The central novel characteristic is the use of a scoring function V of the state, which biases the values of the aggregate cost function towards their correct levels. Different choices for V yield a variety of interesting methods (the classical aggregation framework is obtained when V=0). When V is the cost function of some known policy, our scheme is equivalent to enhanced forms of the rollout algorithm and model predictive control. More generally, our scheme is equivalent to approximation in value space with lookahead function equal to V plus local corrections that are constant within each aggregate state. It can yield an arbitrarily close approximation to the optimal cost function, assuming a sufficiently large number of aggregate states are used.

References (available from the author’s website):
		    
D. P. Bertsekas, <a href="http://www.mit.edu/~dimitrib/Score-Based_Aggregation.pdf">"Biased Aggregation, Rollout, and Enhanced Policy Improvement for Reinforcement Learning</a>," Lab. for Information and Decision Systems Report, MIT, October 2018. 

D. P. Bertsekas, <a href="http://www.mit.edu/~dimitrib/Feature_Aggregation_DeepRL.pdf">"Feature-Based Aggregation and Deep Reinforcement Learning: A Survey and Some New Implementations</a>," Lab. for Information and Decision Systems Report, MIT, April 2018;  <a href="http://www.mit.edu/~dimitrib/Aggregation_Journal_JAS/IEEE_2018.pdf">a version to appear in IEEE/CAA Journal of Automatica Sinica.</a></p>
            <p><strong>Bio: </strong>Dr. Bertsekas has held faculty positions in several universities, including Stanford University (1971-1974) and the University of Illinois, Urbana (1974-1979). Since 1979 he has been teaching at the Electrical Engineering and Computer Science Department of the Massachusetts Institute of Technology, where he is currently McAfee Professor of Engineering. 

Professor Bertsekas was awarded the INFORMS 1997 Prize for Research Excellence in the Interface Between Operations Research and Computer Science for his book "Neuro-Dynamic Programming" (co-authored with John Tsitsiklis), the 2000 Greek National Award for Operations Research, the 2001 ACC John R. Ragazzini Education Award, the 2009 INFORMS Expository Writing Award, the 2014 ACC Richard E. Bellman Control Heritage Award, the 2014 INFORMS Khachiyan Prize, and the SIAM/MOS 2015 George B. Dantzig Prize, and the 2018 INFORMS John von Neumann Theory Prize (jointly with John Tsitsiklis), for the contributions of the research monographs "Parallel and Distributed Computation" and "Neuro-Dynamic Programming". In 2001, he was elected to the United States National Academy of Engineering.

Dr. Bertsekas' recent books are "Convex Optimization Algorithms" (2015), “Nonlinear Programming” (3rd edition, 2016), "Dynamic Programming and Optimal Control” (4th edition, 2017), and "Abstract Dynamic Programming” (2nd edition, 2018), all published by Athena Scientific.
		    </div>
			
			<p> 9:20 - 9:55 <a href="javascript:toggleAbstract('Lewis')">Reinforcement Learning Structures for Real-Time Optimal Control and Differential Games</a> -
			<a href="https://www.uta.edu/utari/acs/">Frank L. Lewis</a> (University of Texas at Arlington) </p>
            <div id="Lewis" style="display:none">
            <p><strong>Abstract: </strong> This talk will discuss some new adaptive control structures for learning online the solutions to optimal control problems and multi-player differential games.  Techniques from reinforcement learning are used to design a new family of adaptive controllers based on actor-critic mechanisms that converge in real time to optimal control and game theoretic solutions.  Continuous-time systems are considered.  Application of reinforcement learning to continuous-time (CT) systems has been hampered because the system Hamiltonian contains the full system dynamics. Using our technique known as Integral Reinforcement Learning (IRL), we will develop reinforcement learning methods that do not require knowledge of the system drift dynamics.  In the linear quadratic (LQ) case, the new RL adaptive control algorithms learn the solution to the Riccati equation by adaptation along the system motion trajectories.  In the case of nonlinear systems with general performance measures, the algorithms learn the (approximate smooth local) solutions of HJ or HJI equations.  New algorithms will be presented for solving online the non zero-sum and zero-sum multi-player games. Each player maintains two adaptive learning structures, a critic network and an actor network.  The result is an adaptive control system that learns based on the interplay of agents in a game, to deliver true online gaming behavior. A new Experience Replay technique is given that uses past data for present learning and significantly speeds up convergence. New methods of Off-policy Learning allow learning of optimal solutions without knowing any dynamic information. New RL methods in Optimal Tracking allow solution of the Output Regulator Equations for heterogeneous multi-agent systems.</p>
			<p><strong>Bio: </strong> Member, National Academy of Inventors.  Fellow IEEE, Fellow IFAC, Fellow AAAS, Fellow U.K. Institute of Measurement & Control, PE Texas, U.K. Chartered Engineer. UTA Distinguished Scholar Professor, UTA Distinguished Teaching Professor, and Moncrief-O’Donnell Chair at The University of Texas at Arlington Research Institute.  Qian Ren Thousand Talents Consulting Professor, Northeastern University, Shenyang, China. Foreign Expert Scholar, Huazhong University of Science and Technology. IEEE Control Systems Society Distinguished Lecturer.  Bachelor's Degree in Physics/EE and MSEE at Rice University, MS in Aeronautical Engineering at Univ. W. Florida, Ph.D. at Ga. Tech.  He works in feedback control, reinforcement learning, intelligent systems, and distributed control systems.  He is author of 7 U.S. patents, 384 journal papers, 426 conference papers, 20 books, 48 chapters, and 12 journal special issues.  He received the Fulbright Research Award, NSF Research Initiation Grant, ASEE Terman Award, Int. Neural Network Soc. Gabor Award 2009, U.K. Inst. Measurement & Control Honeywell Field Engineering Medal 2009.  Received IEEE Computational Intelligence Society Neural Networks Pioneer Award 2012 and AIAA Intelligent Systems Award 2016.  Distinguished Foreign Scholar at Nanjing Univ. Science & Technology. Project 111 Professor at Northeastern University, China. Distinguished Foreign Scholar at Chongqing Univ. China. Received Outstanding Service Award from Dallas IEEE Section, selected as Engineer of the Year by Ft. Worth IEEE Section.  Listed in Ft. Worth Business Press Top 200 Leaders in Manufacturing. Received the 2010 IEEE Region 5 Outstanding Engineering Educator Award and the 2010 UTA Graduate Dean’s Excellence in Doctoral Mentoring Award. Elected to UTA Academy of Distinguished Teachers 2012.  Texas Regents Outstanding Teaching Award 2013. He served on the NAE Committee on Space Station in 1995.  </p>
            </div>

            <p> 9:55 - 10:30 
            <a href="javascript:toggleAbstract('Recht')">The Merits of Models in Continuous Reinforcement Learning</a> - 
			<a href="https://people.eecs.berkeley.edu/~brecht/">Benjamin Recht</a> (University of California, Berkeley)  </p>
            <div id="Recht" style="display:none">
            <p><strong>Abstract: </strong> Classical control theory and machine learning have similar goals: acquire data about the environment, perform a prediction, and use that prediction to positively impact the world. However, the approaches they use are frequently at odds. Controls is the theory of designing complex actions from well-specified models, while machine learning makes intricate, model-free predictions from data alone.  For contemporary autonomous systems, some sort of hybrid may be essential in order to fuse and process the vast amounts of sensor data recorded into timely, agile, and safe decisions.  

In this talk, I will examine the relative merits of model-based and model-free methods in data-driven control problems. I will discuss quantitative estimates on the number of measurements required to achieve a high quality control performance and statistical techniques that can distinguish the relative power of different methods. I will also describe how notions of robustness, safety, constraint satisfaction, and exploration can be transparently incorporated in model-based methods. Given these facts, it will remain unclear what model-free methods have to offer, given their high sample complexity and lack of reliability and versatility.</p>
		<p><strong>Bio: </strong> Benjamin Recht is an Associate Professor in the Department of Electrical Engineering and Computer Sciences at the University of California, Berkeley.  Ben's research group studies the theory and practice of optimization algorithms with a particular focus on applications in machine learning, control systems, and data analysis. Ben is the recipient of a Presidential Early Career Awards for Scientists and Engineers, an Alfred P. Sloan Research Fellowship, the 2012 SIAM/MOS Lagrange Prize in Continuous Optimization, the 2014 Jamon Prize, the 2015 William O. Baker Award for Initiatives in Research, and the 2017 NIPS Test of Time Award.</p>
            </div>
            </td>
			
          </tr>
		  
		  <tr>
            <td> 10:30 - 11:00 </td>
            <td> Break
			<br>
			</td>
		</tr>
		
		<tr>
            <td> 11:00 - 12:00 </td>
            <td> <p><strong>Keynote Session</strong> - Chair: Manfred Morari</p>
			<a href="javascript:toggleAbstract('Keynote')"> Dynamical, Symplectic and Stochastic Perspectives on Gradient-Based Optimization</a> -
			<a href="https://people.eecs.berkeley.edu/~jordan/">Michael I. Jordan</a> (University of California, Berkeley)
            </p>
            <div id="Keynote" style="display:none">
            <p><strong>Abstract: </strong> Many new theoretical challenges have arisen in the area of gradient-based
optimization for large-scale control and inference problems, driven by the needs of
applications and the opportunities provided by new hardware and software platforms.
I discuss several recent, related results in this area: (1) a new framework
for understanding Nesterov acceleration, obtained by taking a continuous-time,
Lagrangian/Hamiltonian/symplectic perspective, (2) a discussion of how to
escape saddle points efficiently in nonconvex optimization, and (3) the
acceleration of Langevin diffusion.</p>

<p><strong>Bio:</strong> Michael I. Jordan is the Pehong Chen Distinguished Professor in the
Department of Electrical Engineering and Computer Science and the
Department of Statistics at the University of California, Berkeley.
His research interests bridge the computational, statistical, cognitive
and biological sciences.  Prof. Jordan is a member of the National Academy
of Sciences and a member of the National Academy of Engineering.
He has been named a Neyman Lecturer and a Medallion Lecturer by the
Institute of Mathematical Statistics.  He received the IJCAI Research
Excellence Award in 2016, the David E. Rumelhart Prize in 2015 and
the ACM/AAAI Allen Newell Award in 2009</p>
            </div>
			</td>
		</tr>
		<tr>
            <td> 12:00 - 1:30 </td>
            <td> Lunch Break
			<br>
			</td>
		</tr>
		
		<tr>
            <td> 1:30 - 3:15 </td>
            <td> <p><strong>Session 2: Optimization and Statistical Learning</strong> - Chair: George J. Pappas</p>
			<!--</td>
		</tr>
		
            <tr>
			<td>  </td>
			<td> -->
			<p> 1:30 - 2:05 
			<a href="javascript:toggleAbstract('Vidal')">Title</a> - 
			<a href="http://cis.jhu.edu/~rvidal/">Rene Vidal</a> (Johns Hopkins University)
            </p>
            <div id="Vidal" style="display:none">
            <p><strong>Abstract: </strong> here</p>
            </div>
			
			<p> 2:05 - 2:40 
            <a href="javascript:toggleAbstract('Fazel')">Convergence of Policy Gradient Methods for the Linear Quadratic Regulator</a> - 
			<a href="https://faculty.washington.edu/mfazel/">Maryam Fazel</a> (University of Washington)
			</p>
            <div id="Fazel" style="display:none">
            <p><strong>Abstract: </strong> Policy gradient methods for reinforcement learning and continuous control are popular in practice, but lack theoretical guarantees even for the simplest case of linear dynamics and a quadratic cost, i.e., the Linear Quadratic Regulator (LQR) problem. A difficulty is that unlike the classical approaches, these methods must solve a nonconvex optimization problem to find the optimal control policy. We show that despite the nonconvexity, gradient descent starting from a stabilizing policy converges to the globally optimal policy. We then discuss how this can help understand policy gradient type methods that do not have access to exact gradients. </p>
		<p><strong>Bio: </strong>Maryam Fazel is an Associate Professor of Electrical Engineering at the University of Washington, with adjunct appointments in Computer Science and Engineering, Mathematics, and Statistics. Maryam received her MS and PhD from Stanford University, her BS from Sharif University in Iran, and was a postdoctoral scholar at Caltech before joining UW. Her current interests are in mathematical optimization and applications in machine learning. She is a recipient of the NSF Career Award, UWEE Outstanding Teaching Award, UAI conference Best Student Paper Award with her student, and coauthored a paper selected as a Fast-Breaking paper by Science Watch in 2011. She co-leads the Algorithmic Foundations for Data Science Institute (ADSI)---an NSF TRIPODS Institute at UW, and is an associate editor of SIAM Journals on Optimization (SIOPT) and on Mathematics of Data Science (SIMODS). 
		    </p>
            </div>

            <p> 2:40 - 3:15 
            <a href="javascript:toggleAbstract('Calafiore')">Scenario Optimization for Robust Design - foundations and recent developments</a> - 
			<a href="http://staff.polito.it/giuseppe.calafiore/Home_Page/Home.html">Giuseppe Carlo Calafiore</a> (Politecnico di Torino)
			</p>
            <div id="Calafiore" style="display:none">
            <p><strong>Abstract: </strong>Random convex programs (RCPs) are convex optimization problems subject to a finite number  of constraints (scenarios)
that are extracted according to some probability distribution. The optimal objective value of an RCP and its associated
optimal solution (when it exists), are random variables: RCP theory is mainly concerned with providing probabilistic
assessments on the objective and on the probability of constraint violation for RCPs.
In this talk, we  give a synthetic overview of RCP theory, discuss practical impact, and illustrate some applicative examples, 
		    with focus on control applications. Finally, we glimpse at recent developments of scenario theory such as 
		    iterative scenario design and non-convex scenario optimization. </p>
		    <p><strong>Bio: </strong> Giuseppe C. Calafiore received the ``Laurea'' degree in Electrical Engineering from Politecnico di Torino in 1993, and the Ph.D. degree in Information and System Theory from Politecnico di Torino, in 1997. He is with the faculty of Dipartimento di Electronics and Telecommunications, Politecnico di Torino, where he currently serves as a full professor and coordinator of the Systems and Data Science lab.
Dr. Calafiore held several visiting positions at international institutions: at the Information Systems Laboratory (ISL), Stanford University, California, in 1995; at the Ecole Nationale Supérieure de Techniques Avanceés (ENSTA), Paris, in 1998; and at the University of California at Berkeley, in 1999, 2003 and 2007. He had an appointment as a Senior Fellow at the Institute of Pure and Applied Mathematics (IPAM), University of California at Los Angeles, in 2010. He had appointments as a Visiting Professor at EECS UC Berkeley in 2017 and at the Haas Business School in 2018.
He is a Fellow of the Italian National Research Council (CNR). He has been an Associate Editor for the IEEE Transactions on Systems, Man, and Cybernetics (T-SMC), for the IEEE Transactions on Automation Science and Engineering (T-ASE), and for the IEEE Transactions on Automatic Control. Dr. Calafiore is the author of more than 180 journal and conference proceedings papers, and of eight books. He is a fellow member of the IEEE since 2018. He received the IEEE Control System Society ``George S. Axelby'' Outstanding Paper Award in 2008.  His research interests are in the fields of convex optimization, randomized algorithms, machine learning, computational finance, and identification and control of uncertain systems.</p>
            </div>
            </td>
			
          </tr>
		  
		  <tr>
            <td> 3:15 - 3:30 </td>
            <td> Break
			<br>
			</td>
		</tr>
		
		  <tr>
            <td> 3:30 - 5:15 </td>
            <td> <p><strong>Session 3: Safe Learning for Control</strong> - Chair: Konstantinos Gatsis</p>
			<!--<br>
			</td>
		</tr>
		
            <tr>
			<td>  </td>
			<td> -->
			<p> 3:30 - 4:05 
            <a href="javascript:toggleAbstract('Schoellig')">Safe model-based learning for robot control</a> -
			<a href="http://www.dynsyslab.org/prof-angela-schoellig/">Angela Schoellig</a> (University of Toronto)
			</p>
            <div id="Schoellig" style="display:none">
            <p><strong>Abstract:</strong> In contrast to computers and smartphones, the promise of robotics is to design devices that can physically interact with the world. Envisioning robots to work in human-centered and interactive environments challenges current robot algorithm design, which has largely been based on a-priori knowledge about the system and its environment. In this talk, we will show how we combine models and data to achieve safe and high-performance robot behavior in the presence of uncertainties and unknown effects. In particular, we combine learned models in the form of Gaussian processes with classic tools from stability theory in order to analyze the stability of a controller on the learned model. Next, we combine this with model predictive control in order to obtain a control algorithm that is provably safe during the learning process. We demonstrate these algorithms on several experiments with self-driving vehicles. More information and videos at: www.dynsyslab.org and https://berkenkamp.me Authors: Felix Berkenkamp and Angela Schoellig
</p>
            </div>
			
			<p> 4:05 - 4:40 
            <a href="javascript:toggleAbstract('Borrelli')">Learning Model Predictive Control</a> - 
			<a href="http://www.me.berkeley.edu/people/faculty/francesco-borrelli">Francesco Borrelli</a> (University of California, Berkeley)
			</p>
            <div id="Borrelli" style="display:none">
            <p><strong>Abstract: </strong> Forecasts play an important role in autonomous and semi-autonomous systems. Applications include transportation, energy, manufacturing and healthcare systems.  Predictions of systems dynamics, human behavior and environment conditions can improve safety and performance of the resulting system. However, constraint satisfaction, performance guarantees and real-time computation are challenged by the growing complexity of the engineered system, the human/machine interaction and the uncertainty of the environment where the system operates.
Our research over the past years has focused on predictive control design for autonomous systems safely performing iterative tasks.  In this talk I will focus on recent results on the use of data to efficiently formulate predictive control problems which safely improve performance in iterative tasks.  Throughout the talk I will focus on autonomous cars to motivate our research and show the benefits of the proposed techniques.
More info on: www.mpc.berkeley.edu
</p>
            </div>

            <p> 4:40 - 5:15 
            <a href="javascript:toggleAbstract('Tomlin')">Safe Learning in Robotics</a> - 
			<a href="https://people.eecs.berkeley.edu/~tomlin/">Claire J. Tomlin</a> (University of California, Berkeley)
			</p>
            <div id="Tomlin" style="display:none">
            <p><strong>Abstract: </strong> A great deal of research in recent years has focused on robot learning.  In many applications, guarantees that specifications are satisfied throughout the learning process are paramount. For the safety specification, we present a controller synthesis technique based on the computation of reachable sets, using optimal control and game theory.  In the first part of the talk, we will review these methods and their application to collision avoidance and avionics design in air traffic management systems, and networks of unmanned aerial vehicles.  In the second part, we will present a toolbox of methods combining reachability with data-driven techniques inspired by machine learning, to enable performance improvement while maintaining safety. We will illustrate these “safe learning” methods on a quadrotor UAV experimental platform which we have at Berkeley, including demonstrations of motion planning around people. 
</p>
            </div>
			
			
            </div>
			
            </td>
			
          </tr>
		  
		  <tr>
            <td> 5:15 - 5:30 </td>
            <td> <strong>Closing remarks by organizers</strong> 

            </td>
          </tr> 
		  <!--
          <tr>
            <td> 9:30 - 10:00 </td>
            <td> 
            <p><strong>Invited talk:</strong> <a href="https://link">Name</a> (University)<br/>
            <a href="javascript:toggleAbstract('Name')">Title</a></p>
            <div id="Name" style="display:none">
            <p><strong>Abstract: </strong> here</p>
            </div>
            </td>
          </tr>
          
          <tr>
            <td> 10:30 - 11:00 </td>
            <td> <strong>Coffee break</strong> </td>
          </tr>
          
         
          <tr>
            <td> 12:30 - 2:00 </td>
            <td> <strong>Lunch break</strong> </td>
          </tr>
          
          <tr>
            <td> 4:30 - 5:00 </td>
            <td> <strong>Afternoon wrap-up: panel discussion & closing remarks</strong> 

            </td>
          </tr>    
           -->            
        </tbody>
      </table>
        
      
      
      

      <!------------ Organizing Committee ----------->
      <div class="page-header" id="contact">
        <h2>Organizers</h2>
      </div>

	  <figure>
		<img src="assets/fig/Gatsis.jpg" alt="Konstantinos Gatsis">
    <figcaption><a href="https://www.seas.upenn.edu/~kgatsis/">Konstantinos Gatsis</a><br />University of Pennsylvania</figcaption>
</figure>
<figure>
		<img src="assets/fig/Khargonekar.jpg" alt="Khargonekar">
    <figcaption><a href="http://faculty.sites.uci.edu/khargonekar/">Pramod P. Khargonekar</a> <br/>University of California, Irvine</figcaption>
</figure>
	 <figure>
		<img src="assets/fig/Morari.jpg" alt="Morari">
    <figcaption><a href="https://www.seas.upenn.edu/directory/profile.php?ID=213">Manfred Morari</a><br />University of Pennsylvania</figcaption>
</figure>

	 <figure>
		<img src="assets/fig/Pappas.jpg" alt="Pappas">
    <figcaption><a href="https://www.georgejpappas.org/">George J. Pappas</a><br />University of Pennsylvania</figcaption>
</figure>


      <!-- <ul>
		<img src="assets/fig/Gatsis.jpg" alt="Konstantinos Gatsis" width="150px" height="150px" >
        <li><a href="https://www.seas.upenn.edu/~kgatsis/">Konstantinos Gatsis</a> (University of Pennsylvania)</li>

		<img src="assets/fig/Khargonekar.jpg" alt="" width="150px" height="150px" style="margin: 0px 10px 0px 0px;">
        <li><a href="http://faculty.sites.uci.edu/khargonekar/">Pramod P. Khargonekar</a> (University of California, Irvine)</li>

		<img src="assets/fig/Morari.jpg" alt="" width="150px" height="150" style="margin: 0px 10px 0px 0px;">
        <li><a href="https://www.seas.upenn.edu/directory/profile.php?ID=213">Manfred Morari</a> (University of Pennsylvania)</li>

		<img src="assets/fig/Pappas.jpg" alt="" width="150px" height="150" style="margin: 0px 10px 0px 0px;">
        <li><a href="https://www.georgejpappas.org/">George J. Pappas</a> (University of Pennsylvania)</li>
      </ul> -->
      <p> Should you have any questions, please do not hesitate to contact the organizers. 
        <!--Please include ``CDC 2016 Workshop Submission'' in the subject of the email. -->
      </p>

      
      
      
    </div><!-- /.container -->


    <!-- Bootstrap core JavaScript
    ================================================== -->
    <!-- Placed at the end of the document so the pages load faster -->
    <script src="https://ajax.googleapis.com/ajax/libs/jquery/1.11.3/jquery.min.js"></script>
    <script>window.jQuery || document.write('<script src="../../assets/js/vendor/jquery.min.js"><\/script>')</script>
    <script src="assets/js/bootstrap.min.js"></script>
    <!-- IE10 viewport hack for Surface/desktop Windows 8 bug -->
    <script src="assets/js/ie10-viewport-bug-workaround.js"></script>
  </body>
</html>
